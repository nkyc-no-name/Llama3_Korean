{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '1 (Python 3.9.19)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n 1 ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "%pip install --no-deps xformers[cuda] \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/kwb425/projects/lina/track3/llama3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "current_directory = os.getcwd()\n",
        "print(current_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlp3KTaWEbrC",
        "outputId": "94e4cb8a-7bd6-4559-b378-604bd7ee629d",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '1 (Python 3.9.19)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n 1 ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Kill all processes running on GPU\n",
        "sh = \"\"\"\n",
        "pids=$(nvidia-smi --query-compute-apps=pid --format=csv,noheader,nounits)\n",
        "\n",
        "# Kill each process\n",
        "for pid in $pids; do\n",
        "    sudo kill -9 $pid\n",
        "done\n",
        "\"\"\"\n",
        "with open('script.sh', 'w') as file:\n",
        "    file.write(sh)\n",
        "\n",
        "!bash script.sh\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.\n",
        "* [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "703b8573-620a-44d2-d135-e29b6ee4119b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "190fd20c7c4a463a8395d6af67af814c",
            "a16786f3db344ae180bd3591cdbbeb90",
            "4322eb285a1c4f6a95cbd28786f9d094",
            "f413aab3dac442bdab8f3f3a0ff1a57d",
            "4e36f8343db046e9aae1bae6b83e516a",
            "5064eb6289fc48b29bb4521ba47fd93b",
            "4dd938c17e5345c8a7182ffcfcecea3d",
            "183d1e62c07249878bea4755a2490f65",
            "f71354d2656d4cf2a7ddd2cc38034d7b",
            "7431b92b260b473f93130362a0ab7cc5",
            "b2d7fa58fb004c608928c2034051cfc2"
          ]
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "72479247-b2a9-4b88-8fb0-c9e4de69547f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
            "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "190fd20c7c4a463a8395d6af67af814c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQMOP5wKZpUg"
      },
      "source": [
        "### Test Inference before gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuYkcW1Y53Pl"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# added, otherwise, error\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        ")\n",
        "###############\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"7대 고액암에 대해 설명해줘!\"},\n",
        "    {\"from\": \"gpt\", \"value\": \"보험약관에서 정한 암 중 백혈병, 뇌암, 골수암, 식도암, 담낭암, 담도암, 췌장암을 말합니다.\"},\n",
        "    {\"from\": \"human\", \"value\": \"유방암은 포함 안 돼?\"}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9Tu88mH9KsL",
        "outputId": "188fe0a7-beaf-4dd7-9e98-2561148b6086"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "7대 고액암에 대해 설명해줘!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "보험약관에서 정한 암 중 백혈병, 뇌암, 골수암, 식도암, 담낭암, 담도암, 췌장암을 말합니다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "유방암은 포함 안 돼?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "유방암은 女성암 중 한 가지입니다. 보험약관에서 정한 여성암은 유방암, 생식암(자궁내막암, 자궁근종, 자궁neck암, 난소암, 생식관암), 기타피부암을 말합니다. <|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgadjC-jZ48-"
      },
      "source": [
        "### Gradio Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZE9yCvDD4w9K",
        "outputId": "5d431603-6219-4199-8d49-4d3984486dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7862\n",
            "Running on public URL: https://9319372a66bb20bb07.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://9319372a66bb20bb07.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-15 (_fast_generate):\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
            "    _threading_Thread_run(self)\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/unsloth/models/llama.py\", line 1194, in _fast_generate\n",
            "    output = generate(*args, **kwargs)\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1989, in generate\n",
            "    result = self._sample(\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2932, in _sample\n",
            "    outputs = self(**model_inputs, return_dict=True)\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/unsloth/models/llama.py\", line 857, in _CausalLM_fast_forward\n",
            "    outputs = fast_forward_inference(\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/unsloth/models/llama.py\", line 811, in LlamaModel_fast_forward_inference\n",
            "    hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/unsloth/models/llama.py\", line 180, in LlamaAttention_fast_forward_inference\n",
            "    Kn = fast_linear_forward(self.k_proj, Xn, out = self.temp_KV[0])\n",
            "  File \"/home/hosungnam/Applications/miniconda3/envs/1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1709, in __getattr__\n",
            "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
            "AttributeError: 'LlamaAttention' object has no attribute 'temp_KV'. Did you mean: 'temp_O'?\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from threading import Thread\n",
        "from transformers import TextIteratorStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        ")\n",
        "\n",
        "def predict(message, history):\n",
        "\n",
        "    history_transformer_format = history + [[message, \"\"]]\n",
        "\n",
        "    messages = [{\"from\": \"system\", \"value\": \"You are a helpful AI assistant. Please follow the user's request kindly.\"}]\n",
        "    for item in history_transformer_format:\n",
        "        messages = messages + [{\"from\": \"human\", \"value\": item[0]}]\n",
        "        messages = messages + [{\"from\": \"gpt\", \"value\": item[1]}]\n",
        "\n",
        "    messages[-1]['value'] = \"\"\n",
        "    model_inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    text_streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generate_kwargs = dict(\n",
        "        input_ids = model_inputs,\n",
        "        streamer = text_streamer,\n",
        "        max_new_tokens = 10000,\n",
        "        use_cache = True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    partial_message = \"\"\n",
        "    for new_token in text_streamer:\n",
        "        if new_token != '<':\n",
        "            partial_message += new_token\n",
        "            yield partial_message\n",
        "\n",
        "gr.ChatInterface(predict).launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "183d1e62c07249878bea4755a2490f65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "190fd20c7c4a463a8395d6af67af814c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a16786f3db344ae180bd3591cdbbeb90",
              "IPY_MODEL_4322eb285a1c4f6a95cbd28786f9d094",
              "IPY_MODEL_f413aab3dac442bdab8f3f3a0ff1a57d"
            ],
            "layout": "IPY_MODEL_4e36f8343db046e9aae1bae6b83e516a"
          }
        },
        "4322eb285a1c4f6a95cbd28786f9d094": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_183d1e62c07249878bea4755a2490f65",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f71354d2656d4cf2a7ddd2cc38034d7b",
            "value": 6
          }
        },
        "4dd938c17e5345c8a7182ffcfcecea3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e36f8343db046e9aae1bae6b83e516a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5064eb6289fc48b29bb4521ba47fd93b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7431b92b260b473f93130362a0ab7cc5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a16786f3db344ae180bd3591cdbbeb90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5064eb6289fc48b29bb4521ba47fd93b",
            "placeholder": "​",
            "style": "IPY_MODEL_4dd938c17e5345c8a7182ffcfcecea3d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b2d7fa58fb004c608928c2034051cfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f413aab3dac442bdab8f3f3a0ff1a57d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7431b92b260b473f93130362a0ab7cc5",
            "placeholder": "​",
            "style": "IPY_MODEL_b2d7fa58fb004c608928c2034051cfc2",
            "value": " 6/6 [00:17&lt;00:00,  2.68s/it]"
          }
        },
        "f71354d2656d4cf2a7ddd2cc38034d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
