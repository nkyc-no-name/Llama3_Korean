{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9hvtdgZoEi6Pc8y1M1C41",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkyc-no-name/Llama3_Korean/blob/main/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "BtVdQkExTmCF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tokenizers import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "] # More models at https://huggingface.co/unsloth"
      ],
      "metadata": {
        "id": "esJK740-VNU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode(\"안내판\"))\n",
        "tokenizer.decode([113698])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg1Zhf_TWxHk",
        "outputId": "3d4a9569-f5d9-473a-c4ed-2b4c09ffbb07"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[113698, 103079]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안내'"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = tokenizer.vocab_size\n",
        "tokens = [tokenizer.decode(i) for i in range(n)]\n",
        "tokens[-100:]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uCv3LF6ZhZ2",
        "outputId": "65abc4b0-031f-439b-c7de-e36488e1f445"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' 바빠서',\n",
              " ' 별관',\n",
              " ' 택한',\n",
              " ' 확인서',\n",
              " ' 안내판',\n",
              " ' 발돋움',\n",
              " ' 강기',\n",
              " ' 선입',\n",
              " ' 챕터',\n",
              " ' 수양',\n",
              " '레지스트',\n",
              " '메스',\n",
              " ' 연사',\n",
              " ' 요것',\n",
              " ' 낙후',\n",
              " ' 이월',\n",
              " ' 당량',\n",
              " ' 가했',\n",
              " '보우',\n",
              " ' 메틸렌',\n",
              " ' 쉴드',\n",
              " ' 쓰러',\n",
              " ' 타이트',\n",
              " ' 뉴스룸',\n",
              " ' 발렌시아',\n",
              " ' 잡아먹',\n",
              " ' 진원',\n",
              " ' 흑색',\n",
              " ' 자며',\n",
              " ' 스테아',\n",
              " ' 포토레지스트',\n",
              " ' 판재',\n",
              " '세척기',\n",
              " '공개',\n",
              " '다음',\n",
              " ' 산전',\n",
              " ' 겨루',\n",
              " ' 어지럽',\n",
              " ' 알약',\n",
              " ' 맘껏',\n",
              " ' 차감',\n",
              " ' 이토',\n",
              " ' 굵직',\n",
              " ' 패소',\n",
              " ' 사교',\n",
              " '퉁이',\n",
              " ' 여객기',\n",
              " ' 복종',\n",
              " ' 회신',\n",
              " ' 톱스타',\n",
              " ' 활기찬',\n",
              " ' 시에',\n",
              " ' 돌려서',\n",
              " ' 플렉서블',\n",
              " ' 김종민',\n",
              " '스턴트',\n",
              " ' 연주회',\n",
              " ' 신다는',\n",
              " ' 피막',\n",
              " ' 장타',\n",
              " ' 당진시',\n",
              " ' 잠복',\n",
              " ' 방석',\n",
              " ' 김두',\n",
              " ' 태권도',\n",
              " ' 심의회',\n",
              " '댔',\n",
              " ' 물통',\n",
              " ' 사나이',\n",
              " ' 한솔',\n",
              " ' 스케줄링',\n",
              " ' 돌아온다',\n",
              " ' 눌러서',\n",
              " ' 주워',\n",
              " '섹',\n",
              " ' 잡초',\n",
              " ' 윌리엄스',\n",
              " ' 명나라',\n",
              " ' 컴팩트',\n",
              " ' 고해상도',\n",
              " ' 거처',\n",
              " '더슨',\n",
              " ' 나만',\n",
              " ' 후원자',\n",
              " ' 맨투맨',\n",
              " ' 고함',\n",
              " ' 로렌',\n",
              " ' 하객',\n",
              " '이상',\n",
              " ' 이음',\n",
              " ' 오산시',\n",
              " ' 익힌',\n",
              " ' 신축성',\n",
              " ' 곰탕',\n",
              " ' 참여연대',\n",
              " '존스',\n",
              " ' 모하',\n",
              " ' 식기세척기',\n",
              " ' 연합군',\n",
              " ' 짭짤']"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode(\" 안내판\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWlpyfP4fHpf",
        "outputId": "ec3ad296-3a4c-48b9-fa51-3d9d1687b88d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[145695]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"saltlux/Ko-Llama3-Luxia-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "[tokenizer.decode([id]) for id in tokenizer.encode(\"본 발명은 금속판의 다수 부분을 에칭시켜 특정 무늬모양을 형성하는 건축용 금속재 장식판으로 이루어진 것에 특징이 있다.\")]"
      ],
      "metadata": {
        "id": "0LoJpSEHU38X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "[tokenizer.decode([id]) for id in tokenizer.encode(\"본 발명은 금속판의 다수 부분을 에칭시켜 특정 무늬모양을 형성하는 건축용 금속재 장식판으로 이루어진 것에 특징이 있다.\")]"
      ],
      "metadata": {
        "id": "T5XZz0YjUohW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "[tokenizer.decode([id]) for id in tokenizer.encode(\"본 발명은 금속판의 다수 부분을 에칭시켜 특정 무늬모양을 형성하는 건축용 금속재 장식판으로 이루어진 것에 특징이 있다.\")]"
      ],
      "metadata": {
        "id": "Okw1qcGRV0bJ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "text = \"안녕하세요, 네네. 저는 인공지능입니다.\"\n",
        "\n",
        "# Tokenize text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculations for inference\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBwr418ifY3D",
        "outputId": "8d56ab6d-d10d-4459-ccdf-06add1e9bf85"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
            "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)\n",
        "outputs.logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KHHHv7gmgb2",
        "outputId": "2e908d71-7cbb-4ff5-a9d3-65cf405d0e08"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[128000, 101193, 124409,     11, 103315, 101886,     13, 102678,  16969,\n",
            "          59777, 103896,  67119,  80052,     13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 14, 128256])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    }
  ]
}